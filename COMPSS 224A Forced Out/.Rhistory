# permnos
# events_gvkey <- events_gvkey %>% rename(lpermno = lpermno.y) # fixing original data
permnos <- events_gvkey %>% select(lpermno) %>% drop_na() %>% distinct() %>% pull(lpermno)
# download crsp returns for our sample
crsp <- tbl(wrds, sql("SELECT * FROM crsp.dsf")) %>%
filter(permno %in% permnos) %>%
filter(date >= '1998-01-01') %>%
collect()
# make a function to get the return series for each observations
get_rets <- function(i) {
# get event date
edt <- events_gvkey$event_date[i]
# find id for the closest trading date
trade_date <- min(ff %>% filter(date >= edt) %>% pull(date))
k <- which(ff$date == trade_date)
# get skeleton for dates
skel <- ff[(k - 20):(k + 20), ] %>%
mutate(rel_date = -20:20)
# merge in crsp data
skel <- skel %>%
left_join(crsp %>% filter(permno == events_gvkey$lpermno[i]) %>% select(date, permno, ret))
# make dataset
skel %>%
mutate(row_no = i) %>%
select(row_no, rel_date, date, ret, mktrf)
}
# identify rows to estimate over
rows <- which(!is.na(events_gvkey$lpermno))
# vectorize
out <- map_dfr(rows, get_rets)
plot_data <- out %>%
filter(!is.na(ret)) %>%
group_by(row_no) %>%
filter(n() == 41) %>%
mutate(ret = if_else(rel_date == -20, 0, ret)) %>%
mutate(prodsumret = cumprod(ret + 1) - 1) %>%
# get mean and sd by time period
group_by(rel_date) %>%
summarize(count = n(),
mean = mean(prodsumret),
sd = sd(prodsumret))
plot_data %>%
# make lower and upper confidence intervals
mutate(lower_ci = mean - 1.96 * sd / sqrt(count),
upper_ci = mean + 1.96 * sd / sqrt(count)) %>%
# plot
ggplot(aes(x = rel_date, y = mean)) +
geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 1/4, color = NA) +
geom_point(size = 3) + geom_line(size = 2) +
geom_vline(xintercept = 0, linetype = "dashed") +
scale_y_continuous(labels = scales::percent_format(accuracy = .1)) +
labs(x = "t",
y = "Average \n Cumulative Return") +
geom_hline(yintercept = 0) +
theme(axis.title.y = element_text(vjust = 0.5, angle = 360, size = 14),
axis.title.x = element_text(size = 14),
axis.text = element_text(size = 12),
legend.position = 'bottom',
legend.title = element_blank())
plot_data <- out %>%
filter(!is.na(ret)) %>%
group_by(row_no) %>%
filter(n() == 41) %>%
mutate(ret = if_else(rel_date == -20, 0, ret),
mktrf = if_else(rel_date == -20, 0, mktrf)) %>%
mutate(prodsumret = cumprod(ret + 1) - 1,
prodsummkt = cumprod(mktrf + 1) - 1,
bhar = prodsumret - prodsummkt) %>%
# get mean and sd by time period
group_by(rel_date) %>%
summarize(count = n(),
mean = mean(bhar),
sd = sd(bhar))
plot_data %>%
# make lower and upper confidence intervals
mutate(lower_ci = mean - 1.96 * sd / sqrt(count),
upper_ci = mean + 1.96 * sd / sqrt(count)) %>%
# plot
ggplot(aes(x = rel_date, y = mean)) +
geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 1/4, color = NA) +
geom_point(size = 3) + geom_line(size = 2) +
geom_vline(xintercept = 0, linetype = "dashed") +
scale_y_continuous(labels = scales::percent_format(accuracy = .1)) +
labs(x = "t",
y = "BHAR") +
geom_hline(yintercept = 0) +
theme(axis.title.y = element_text(vjust = 0.5, angle = 360, size = 14),
axis.title.x = element_text(size = 14),
axis.text = element_text(size = 12),
legend.position = 'bottom',
legend.title = element_blank())
# make a function to get the return series for each observations
get_ff_ests <- function(i) {
# get event date
edt <- events_gvkey$event_date[i]
# find id for the closest trading date
trade_date <- min(ff %>% filter(date >= edt) %>% pull(date))
k <- which(ff$date == trade_date)
start <- k - 270
end <- k - 21
# get skeleton for dates
skel <- ff[start:(k + 20), ] %>%
mutate(rel_date = -270:20)
# merge in crsp data
skel <- skel %>%
left_join(crsp %>% filter(permno == events_gvkey$lpermno[i]) %>% select(date, permno, ret))
# require there to be at least 50 returns in estimation period.
nrow <- skel %>% filter(rel_date %>% between(-270, -21)) %>% filter(!is.na(ret)) %>% nrow()
if(nrow < 50) {return(NULL)}
# estimate model
mod <- lm(ret ~ mktrf + smb + hml + umd, data = skel %>% filter(rel_date %>% between(-270, -21)))
# add in predictions to model
skel <- skel %>% mutate(pred = predict(mod, .))
# make dataset
skel %>%
mutate(row_no = i,
sigma = sigma(mod)) %>%
select(row_no, rel_date, date, ret, pred, sigma)
}
# identify rows to estimate over
rows <- which(!is.na(events_gvkey$lpermno))
# vectorize
out <- map_dfr(rows, get_ff_ests)
# make a function to get the return series for each observations
get_ff_ests <- function(i) {
# get event date
edt <- events_gvkey$event_date[i]
# find id for the closest trading date
trade_date <- min(ff %>% filter(date >= edt) %>% pull(date))
k <- which(ff$date == trade_date)
start <- k - 270
end <- k - 21
# get skeleton for dates
skel <- ff[start:(k + 20), ] %>%
mutate(rel_date = -270:20)
# merge in crsp data
skel <- skel %>%
left_join(crsp %>% filter(permno == events_gvkey$lpermno[i]) %>% select(date, permno, ret))
# require there to be at least 50 returns in estimation period.
nrow <- skel %>% filter(rel_date %>% between(-270, -21)) %>% filter(!is.na(ret)) %>% nrow()
if(nrow < 50) {return(NULL)}
# estimate model
mod <- lm(ret ~ mktrf + smb + hml + umd, data = skel %>% filter(rel_date %>% between(-270, -21)))
# add in predictions to model
skel <- skel %>% mutate(pred = predict(mod, .))
# make dataset
skel %>%
mutate(row_no = i,
sigma = sigma(mod)) %>%
select(row_no, rel_date, date, ret, pred, sigma)
}
# identify rows to estimate over
rows <- which(!is.na(events_gvkey$lpermno))
# vectorize
out <- map_dfr(rows, get_ff_ests)
print(nrow(skel))  # 看看 skel 实际有多少行
print(nrow(skel))
# make a function to get the return series for each observations
get_ff_ests <- function(i) {
# get event date
edt <- events_gvkey$event_date[i]
# find id for the closest trading date
trade_date <- min(ff %>% filter(date >= edt) %>% pull(date))
k <- which(ff$date == trade_date)
start <- k - 270
end <- k - 21
# get skeleton for dates
skel <- ff[start:(k + 20), ] %>%
mutate(rel_date = -270:20)
# merge in crsp data
skel <- skel %>%
left_join(crsp %>% filter(permno == events_gvkey$lpermno[i]) %>% select(date, permno, ret))
# require there to be at least 50 returns in estimation period.
nrow <- skel %>% filter(rel_date %>% between(-270, -21)) %>% filter(!is.na(ret)) %>% nrow()
if(nrow < 50) {return(NULL)}
# estimate model
mod <- lm(ret ~ mktrf + smb + hml + umd, data = skel %>% filter(rel_date %>% between(-270, -21)))
# add in predictions to model
skel <- skel %>% mutate(pred = predict(mod, .))
# make dataset
skel %>%
mutate(row_no = i,
sigma = sigma(mod)) %>%
select(row_no, rel_date, date, ret, pred, sigma)
}
# identify rows to estimate over
rows <- which(!is.na(events_gvkey$lpermno))
print(nrow(skel))  # 看看 skel 实际有多少行
print(skel)
# make a function to get the return series for each observations
get_ff_ests <- function(i) {
# get event date
edt <- events_gvkey$event_date[i]
# find id for the closest trading date
trade_date <- min(ff %>% filter(date >= edt) %>% pull(date))
k <- which(ff$date == trade_date)
start <- k - 270
end <- k - 21
# get skeleton for dates
skel <- ff[start:(k + 20), ] %>%
mutate(rel_date = -270:20)
# merge in crsp data
skel <- skel %>%
left_join(crsp %>% filter(permno == events_gvkey$lpermno[i]) %>% select(date, permno, ret))
# require there to be at least 50 returns in estimation period.
nrow <- skel %>% filter(rel_date %>% between(-270, -21)) %>% filter(!is.na(ret)) %>% nrow()
if(nrow < 50) {return(NULL)}
# estimate model
mod <- lm(ret ~ mktrf + smb + hml + umd, data = skel %>% filter(rel_date %>% between(-270, -21)))
# add in predictions to model
skel <- skel %>% mutate(pred = predict(mod, .))
# make dataset
skel %>%
mutate(row_no = i,
sigma = sigma(mod)) %>%
select(row_no, rel_date, date, ret, pred, sigma)
print(nrow(skel))  # 看看 skel 实际有多少行
print(length(-270:20))  # 看看 -270:20 生成的向量长度
}
# identify rows to estimate over
rows <- which(!is.na(events_gvkey$lpermno))
# vectorize
out <- map_dfr(rows, get_ff_ests)
View(events_gvkey)
View(events_gvkey)
write.csv(events_gvkey, "events_gvkey.csv", row.names = FALSE)
write.csv(events_gvkey, "events_gvkey.csv", row.names = FALSE)
getwd()
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(tidyverse)
library(tidycensus)
library(dplyr)
library(ggplot2)
library(sf)
library(tigris)
library(ggthemes)
library("viridis")
# AI Usage
#
# Set Census API Key (Replace with your own key)
census_api_key("63217a192c5803bfc72aab537fe4bf19f6058326", overwrite = TRUE, install = TRUE)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(tidyverse)
library(tidycensus)
library(dplyr)
library(ggplot2)
library(sf)
library(tigris)
library(ggthemes)
library("viridis")
# AI Usage
#
# Set Census API Key (Replace with your own key)
census_api_key("63217a192c5803bfc72aab537fe4bf19f6058326", overwrite = TRUE, install = TRUE)
# Define ACS Variables with Readable Names
acs_vars <- c(
# Migration & Mobility
"B07003_001" = "Total Population (1yr+)",
"B07003_002" = "Same House (No Move)",
"B07003_003" = "Moved Within State (Intrastate)",
"B07003_004" = "Moved to Different State (Interstate)",
"B07003_005" = "Moved from Abroad",
# Foreign-Born Population
"B05002_001" = "Total Foreign-Born Population",
"B05002_002" = "Naturalized U.S. Citizen",
"B05002_003" = "Not a U.S. Citizen",
# Year of Entry for Immigrants
"B05005_001" = "Total Foreign-Born (Year of Entry)",
"B05005_002" = "Entered 2010 or Later",
"B05005_003" = "Entered Before 2010",
# Mobility by Citizenship Status
"B07001_001" = "Total Population (Mobility Status)",
"B07001_017" = "Foreign-Born Movers",
# Migration Flow by Race/Ethnicity
"B07004A_001" = "Migration Flow - White",
"B07004B_001" = "Migration Flow - Black",
"B07004D_001" = "Migration Flow - Asian",
"B07004I_001" = "Migration Flow - Latinx",
# Rent Burden
"B25070_007" = "Rent Burden Rate"
)
# Fetch ACS data for Sub-Districts within San Mateo County (2012-2022)
years <- 2012:2022
acs_data_list <- lapply(years, function(y) {
get_acs(
geography = "tract",
state = "CA",
county = "San Mateo",
variables = names(acs_vars),
year = y,
survey = "acs5"
) %>%
mutate(year = y)  # Add a column for year
})
# Combine data into a single dataframe
acs_data <- bind_rows(acs_data_list)
# Rename the variables using the defined labels
acs_data <- acs_data %>%
mutate(variable = recode(variable, !!!acs_vars))
# View cleaned dataset
print(acs_data)
library(scales)  # For formatting numbers
# Aggregate tract-level data to county level
acs_county_data <- acs_data %>%
group_by(year, variable) %>%
summarize(estimate = sum(estimate, na.rm = TRUE), .groups = "drop")
# Convert year to integer format
acs_county_data$year <- as.integer(acs_county_data$year)
# Define categories for separate plots
categories <- list(
"Migration & Mobility" = c("Total Population (1yr+)", "Same House (No Move)",
"Moved Within State (Intrastate)", "Moved to Different State (Interstate)", "Moved from Abroad"),
"Foreign-Born Population" = c("Total Foreign-Born Population", "Naturalized U.S. Citizen", "Not a U.S. Citizen"),
"Year of Entry for Immigrants" = c("Total Foreign-Born (Year of Entry)", "Entered 2010 or Later", "Entered Before 2010"),
"Mobility by Citizenship Status" = c("Total Population (Mobility Status)", "Foreign-Born Movers"),
"Migration Flow by Race/Ethnicity" = c("Migration Flow - White", "Migration Flow - Black",
"Migration Flow - Asian", "Migration Flow - Latinx")
)
# Function to plot each category separately with viridis colors
plot_migration_trends <- function(category_name, variables) {
# Generate a viridis color palette based on the number of variables
colors <- viridis(length(variables))
ggplot(acs_county_data %>% filter(variable %in% variables),
aes(x = year, y = estimate, color = variable, group = variable)) +
geom_line(size = 1) +  # Line plot
geom_point(size = 2) +  # Add points
scale_color_manual(values = colors) +  # Apply viridis colors
scale_x_continuous(breaks = seq(min(acs_county_data$year), max(acs_county_data$year), by = 1)) +  # Ensure integer years
scale_y_continuous(labels = comma) +  # Format numbers with commas
labs(
title = paste0(category_name, " Trends in San Mateo"),
x = "Year",
y = "Population Estimate",
color = "Category"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
}
# Generate and display five separate plots with viridis colors
plots <- lapply(names(categories), function(cat) {
plot_migration_trends(cat, categories[[cat]])
})
# Display all plots
print(plots[[1]])  # Migration & Mobility
print(plots[[2]])  # Foreign-Born Population
print(plots[[3]])  # Year of Entry for Immigrants
print(plots[[4]])  # Mobility by Citizenship Status
print(plots[[5]])  # Migration Flow by Race/Ethnicity
library(plm)  # For panel data regression
# Step 1: Convert to Wide Format
acs_panel <- acs_data %>%
select(GEOID, year, variable, estimate) %>%  # Keep only necessary columns
pivot_wider(names_from = variable, values_from = estimate)  # Reshape data
# Step 2: Generate Lagged Migration Variables (T-1)
acs_panel <- acs_panel %>%
rename(
RentBurdenRate = `Rent Burden Rate`,
ForeignBornPop = `Total Foreign-Born Population`,
NaturalizedCitizen = `Naturalized U.S. Citizen`,
NonCitizen = `Not a U.S. Citizen`,
ForeignBornEntry = `Total Foreign-Born (Year of Entry)`,
Entered2010Later = `Entered 2010 or Later`,
EnteredBefore2010 = `Entered Before 2010`,
MobilityPop = `Total Population (Mobility Status)`,
ForeignBornMovers = `Foreign-Born Movers`,
MigrationWhite = `Migration Flow - White`,
MigrationBlack = `Migration Flow - Black`,
MigrationAsian = `Migration Flow - Asian`,
MigrationLatinx = `Migration Flow - Latinx`
)
acs_panel <- acs_panel %>%
arrange(GEOID, year) %>%
group_by(GEOID) %>%
mutate(
lag_ForeignBornPop = lag(ForeignBornPop),
lag_NaturalizedCitizen = lag(NaturalizedCitizen),
lag_NonCitizen = lag(NonCitizen),
lag_Entered2010Later = lag(Entered2010Later),
lag_EnteredBefore2010 = lag(EnteredBefore2010),
lag_ForeignBornMovers = lag(ForeignBornMovers),
lag_MigrationWhite = lag(MigrationWhite),
lag_MigrationBlack = lag(MigrationBlack),
lag_MigrationAsian = lag(MigrationAsian),
lag_MigrationLatinx = lag(MigrationLatinx)
) %>%
ungroup()
# View reshaped dataset
print(acs_panel)
panel_model <- plm(
RentBurdenRate ~
lag_ForeignBornPop +
lag_NaturalizedCitizen +
lag_NonCitizen +
lag_Entered2010Later +
lag_EnteredBefore2010 +
lag_ForeignBornMovers +
lag_MigrationWhite +
lag_MigrationBlack +
lag_MigrationAsian +
lag_MigrationLatinx,
data = acs_panel,
index = c("GEOID", "year"),  # Panel structure (tract, year)
model = "within",  # Fixed effects
effect = "twoways"  # Includes both tract and year fixed effects
)
# View Regression Results
summary(panel_model)
library(broom)
# Step 1: Extract regression results
regression_results <- tidy(panel_model) %>%
filter(term != "(Intercept)") %>%  # Remove intercept if present
mutate(
significance = case_when(
p.value < 0.001 ~ "***",
p.value < 0.01  ~ "**",
p.value < 0.05  ~ "*",
p.value < 0.1   ~ ".",
TRUE            ~ ""
)
)
# Step 2: Create coefficient plot with Viridis colors
ggplot(regression_results, aes(x = estimate, y = reorder(term, estimate))) +
geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +  # Reference line
geom_point(aes(color = significance), size = 3) +  # Coefficient points
geom_errorbarh(aes(xmin = estimate - 1.96 * std.error,
xmax = estimate + 1.96 * std.error), height = 0.2) +  # 95% CI
scale_color_viridis_d(option = "magma") +  # Apply Viridis palette ("magma", "plasma", "viridis", "inferno")
labs(
title = "Immigration and Rent Burden Rate",
x = "Coefficient Estimate",
y = "Predictor Variables",
color = "Significance Level"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
legend.position = "bottom"
)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
# 📌 Load necessary libraries
library(tidyverse)
library(tidycensus)
library(tigris)
library(sf)
# 📌 Set Census API Key (Replace with your own key)
census_api_key("63217a192c5803bfc72aab537fe4bf19f6058326", install = TRUE, overwrite = TRUE)
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
# AI Usage
# https://chatgpt.com/share/67a3f535-bcac-8003-8af2-11ef5e7517ee
# 📌 Load necessary libraries
library(tidyverse)
library(tidycensus)
library(tigris)
library(sf)
# 📌 Set Census API Key (Replace with your own key)
census_api_key("63217a192c5803bfc72aab537fe4bf19f6058326", install = TRUE, overwrite = TRUE)
# Define ACS variables to retrieve
acs_vars <- c(
"B19013_001",  # Median Household Income
"B25070_001",  # Households paying 30%+ income on rent
"B25003_002",  # Owner-occupied housing
"B25003_003",  # Renter-occupied housing
"B17001_002"   # Individuals below poverty line
)
# Get ACS data for all counties in California (WITHOUT geometry)
housing_data <- get_acs(
geography = "county",
variables = acs_vars,
state = "CA",
year = 2023,
survey = "acs5"
)
# Retrieve California County Shapefiles using `tigris`
ca_counties <- counties(state = "CA", class = "sf")  # Get geographic shapes
# Reshape ACS data: Pivot variables into columns
housing_cleaned <- housing_data %>%
select(NAME, variable, estimate) %>%  # Keep relevant columns
pivot_wider(names_from = variable, values_from = estimate)
# Check column names before renaming
print(colnames(housing_cleaned))
# Rename columns dynamically
housing_cleaned <- housing_cleaned %>%
rename_with(~ case_when(
. == "B19013_001" ~ "median_income",
. == "B25070_001" ~ "rent_burden",
. == "B25003_002" ~ "homeowners",
. == "B25003_003" ~ "renters",
. == "B17001_002" ~ "poverty",
TRUE ~ .
)) %>%
mutate(
homeownership_rate = homeowners / (homeowners + renters) * 100,
rent_burden_rate = rent_burden / (homeowners + renters) * 100
)
housing_cleaned <- housing_cleaned %>%
mutate(NAME = str_remove(NAME, " County, California$"))
# 📌 Merge ACS data with county geometry using a Left Join
housing_map_data <- left_join(ca_counties, housing_cleaned, by = c("NAME" = "NAME"))
# Plot the California Map - Median Income by County
ggplot(housing_map_data) +
geom_sf(aes(fill = median_income), color = "white", lwd = 0.2) +
scale_fill_viridis_c(option = "viridis", name = "Median Income ($)") +
labs(title = "Median Household Income by County in California (2023)",
subtitle = "Source: American Community Survey 2023 (5-Year Estimates)") +
theme_minimal()
